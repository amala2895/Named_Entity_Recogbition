{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os, sys\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('lookup.json') as fl:\n",
    "    tag_lookup = json.load(fl)\n",
    "    \n",
    "with open('word_list.txt') as fl:\n",
    "    vocab = fl.read().split('\\n')\n",
    "\n",
    "embeds = np.load('word_embeds.npy')\n",
    "wlookup = { word: index for index, word in enumerate(vocab) }\n",
    "\n",
    "len(vocab), embeds.shape, len(tag_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(embeds[wlookup['Japan']]), np.max(embeds[wlookup['Russia']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eng.train') as fl:\n",
    "    sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "with open('eng.testa') as fl:\n",
    "    eval_sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "with open('eng.testb') as fl:\n",
    "    test_sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "len(sents), len(eval_sents), len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate the (embedded representation, 1-hot target predictions) for a given sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_sent(sent):\n",
    "    \n",
    "    words = sent.split('\\n')\n",
    "    inps = []\n",
    "    outs = []\n",
    "    for wordinfo in words:\n",
    "        word, _, _, tag = wordinfo.split()\n",
    "        try:\n",
    "            assert word in wlookup\n",
    "        except:\n",
    "            # word not in our known dictionary, so use the unk token\n",
    "            word = 'unk'\n",
    "        \n",
    "        inps.append(embeds[wlookup[word]])\n",
    "        hot = np.zeros(len(tag_lookup))\n",
    "        hot[tag_lookup[tag]] = 1\n",
    "        outs.append(hot)\n",
    "        \n",
    "    if(len(inps)<124):\n",
    "        for i in range(0,124-len(inps)):\n",
    "            inps.append(np.zeros(300))\n",
    "            hot = np.zeros(len(tag_lookup))\n",
    "            hot[tag_lookup[\"EMPTY\"]] = 1\n",
    "            outs.append(hot)\n",
    "    \n",
    "    return [np.vstack(inps), np.vstack(outs)]\n",
    "\n",
    "ins, outs = load_sent(sents[0])\n",
    "\n",
    "ins.shape, outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def FindMax():\n",
    "    maxx=-1\n",
    "    for i in range(0,len(sents)):\n",
    "        ins, outs = load_sent(sents[i])\n",
    "        if(ins.shape[0]>maxx):\n",
    "            maxx=ins.shape[0]\n",
    "    print(maxx)\n",
    "    for i in range(0,len(eval_sents)):\n",
    "        ins, outs = load_sent(eval_sents[i])\n",
    "        if(ins.shape[0]>maxx):\n",
    "            maxx=ins.shape[0]\n",
    "    print(maxx)\n",
    "    for i in range(0,len(test_sents)):\n",
    "        ins, outs = load_sent(test_sents[i])\n",
    "        if(ins.shape[0]>maxx):\n",
    "            maxx=ins.shape[0]\n",
    "           \n",
    "    print(maxx)\n",
    "    \n",
    "    \n",
    "\n",
    "FindMax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BSIZE = 32\n",
    "train_inds = list(range(len(sents)))\n",
    "shuffle(train_inds)\n",
    "train_inds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, insize=300, outsize=9, hsize=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Dropout\n",
    "        # TODO: nonlinearities\n",
    "        # TODO: Bidirectional\n",
    "\n",
    "        self.hsize = hsize\n",
    "        self.inp = nn.Sequential(\n",
    "            nn.Linear(insize, hsize),\n",
    "        )\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(hsize*2, outsize),\n",
    "            nn.Softmax(dim=-1),\n",
    "        )\n",
    "\n",
    "        # FIXME: this is a uni-directional LSTM\n",
    "        self.rnn = nn.LSTM(hsize, hsize, 1, batch_first=True,bidirectional=True)\n",
    "        \n",
    "    def forward(self, inputs, hidden=None):\n",
    "        hin = self.inp(inputs)\n",
    "        \n",
    "        hout, hidden = self.rnn(hin)\n",
    "        \n",
    "        yout = self.out(hout)\n",
    "        \n",
    "        return yout, hidden\n",
    "    \n",
    "model = RNN().to(device)\n",
    "criterion = nn.MSELoss().cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=0.0005)\n",
    "# sch = optim.lr_scheduler.StepLR(opt, step_size=30, gamma=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPS = 3\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "n2t = lambda narr: torch.from_numpy(narr).to(device).float()\n",
    "\n",
    "def eval_model(evaldata, results=False):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    ypreds = []\n",
    "    for sent in evaldata:\n",
    "        Xs, Ys = zip(*[load_sent(sent)])\n",
    "        Xs, Ys = np.array(Xs), np.array(Ys)\n",
    "        \n",
    "        Xs, Ys = n2t(Xs), n2t(Ys)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat, _ = model(Xs)\n",
    "            \n",
    "            ypreds.append(yhat)\n",
    "            loss = criterion(yhat, Ys)\n",
    "            losses.append(loss.item())\n",
    "    print('Eval: %.4f' % np.mean(losses))\n",
    "    \n",
    "    if results: \n",
    "        return ypreds\n",
    "    else:\n",
    "        return np.mean(losses)\n",
    "    \n",
    "eval_loss.append(eval_model(eval_sents))\n",
    "\n",
    "for epoch in range(EPS):\n",
    "    model.train()\n",
    "    t0 = time()\n",
    "    batch_loss = []\n",
    "    for bi in range(0, len(train_inds)-BSIZE, BSIZE):\n",
    "        inds = train_inds[bi:bi+BSIZE]\n",
    "\n",
    "        # TODO: correct formatting for batchsize >1\n",
    "        Xs, Ys = zip(*[load_sent(sents[ind]) for ind in inds])\n",
    "        Xs, Ys = np.array(Xs), np.array(Ys)\n",
    "        Xs, Ys = n2t(Xs), n2t(Ys)\n",
    "        # shape: (batch x seqlen x dim)\n",
    "        \n",
    "        yhat, _ = model(Xs)\n",
    "\n",
    "        opt.zero_grad()\n",
    "        loss = criterion(yhat, Ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        sys.stdout.write('\\r[E%d/%d - B%d/%d] Train: %.4f ' % (\n",
    "            epoch+1, EPS,\n",
    "            bi+1, len(train_inds),\n",
    "            loss.item(),\n",
    "        ))\n",
    "        batch_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "    sys.stdout.write('(elapsed: %.2fs)\\n' % (time() - t0))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    loss = eval_model(eval_sents)\n",
    "    eval_loss.append(loss)\n",
    "        \n",
    "    shuffle(train_inds)\n",
    "    # TODO: shuffle train inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "plt.plot(train_loss)\n",
    "plt.plot(eval_loss[1:])\n",
    "plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = eval_model(test_sents, results=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "index2tag = { index: tag for tag, index in tag_lookup.items() }\n",
    "with open('nn.output' , 'w') as fl:\n",
    "    assert len(test_results) == len(test_sents)\n",
    "    for ti, (yhat, schunk) in enumerate(zip(test_results, test_sents)):\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "        for wi, wordinfo in enumerate(schunk.split('\\n')):\n",
    "            word, _, _, tag = wordinfo.split()\n",
    "            taghat = index2tag[np.argmax(yhat[0, wi, :])]\n",
    "\n",
    "            fl.write('%s %s %s\\n' % (word, taghat, tag))\n",
    "        fl.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python conlleval.py nn.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "26fJ4SAaBYb7",
    "outputId": "6520485b-8510-429a-f1db-64ad4c1b1baa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aPlImW1FozLf"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torch import nn\n",
    "import json\n",
    "from numpy.random import shuffle\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import os, sys\n",
    "from time import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "op0W9tvYozLi",
    "outputId": "1f4768eb-c32c-4b27-e1ee-68ef5b55b729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YsEc5NXqozLm",
    "outputId": "a41b34ab-f02a-4bf8-ee67-f3c93da9ded3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23625, (23625, 300), 8)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('lookup.json') as fl:\n",
    "    tag_lookup = json.load(fl)\n",
    "    \n",
    "with open('word_list.txt') as fl:\n",
    "    vocab = fl.read().split('\\n')\n",
    "\n",
    "embeds = np.load('word_embeds.npy')\n",
    "wlookup = { word: index for index, word in enumerate(vocab) }\n",
    "\n",
    "len(vocab), embeds.shape, len(tag_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fgoO1UraozLp",
    "outputId": "84b57534-1212-4e4f-c9c8-fdd7a050c748"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.376953125, 0.48046875)"
      ]
     },
     "execution_count": 45,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(embeds[wlookup['Japan']]), np.max(embeds[wlookup['Russia']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NBgXtVPJozLt",
    "outputId": "3d5f3ab6-43da-497b-fac9-b0f61b9cdaac"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14986, 3465, 3683)"
      ]
     },
     "execution_count": 46,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('eng.train') as fl:\n",
    "    sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "with open('eng.testa') as fl:\n",
    "    eval_sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "with open('eng.testb') as fl:\n",
    "    test_sents = fl.read().split('\\n\\n')[1:-1]\n",
    "    \n",
    "len(sents), len(eval_sents), len(test_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "oo809eNnozLy",
    "outputId": "14165bf2-28a5-4b0c-b041-056e6c7c9dde"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9, 300), (9, 8))"
      ]
     },
     "execution_count": 47,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_sent(sent):\n",
    "    words = sent.split('\\n')\n",
    "    inps = []\n",
    "    outs = []\n",
    "    for wordinfo in words:\n",
    "        word, _, _, tag = wordinfo.split()\n",
    "        try:\n",
    "            assert word in wlookup\n",
    "        except:\n",
    "            # word not in our known dictionary, so use the unk token\n",
    "            word = 'unk'\n",
    "        inps.append(embeds[wlookup[word]])\n",
    "        hot = np.zeros(len(tag_lookup))\n",
    "        hot[tag_lookup[tag]] = 1\n",
    "        outs.append(hot)\n",
    "    return [np.vstack(inps), np.vstack(outs)]\n",
    "\n",
    "ins, outs = load_sent(sents[0])\n",
    "ins.shape, outs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "-d53QipcozL1"
   },
   "outputs": [],
   "source": [
    "def getLen(sent):\n",
    "    words = sent.split('\\n')\n",
    "    return len(words)\n",
    "train_len = [getLen(sent) for sent in sents]\n",
    "eval_len = [getLen(sent) for sent in eval_sents]\n",
    "test_len = [getLen(sent) for sent in test_sents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IX5cQpr-ozL8",
    "outputId": "fd36ed40-7ac0-41ea-e6ad-f817ec044176"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10531, 8622, 5422, 14098, 9483, 2019, 639, 11708, 12437, 14660]"
      ]
     },
     "execution_count": 49,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BSIZE = 20\n",
    "train_inds = list(range(len(sents)))\n",
    "shuffle(train_inds)\n",
    "train_inds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "rGITwdoKozL-"
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, insize=300, outsize=8, hsize=128):\n",
    "        super().__init__()\n",
    "        \n",
    "        # TODO: Dropout\n",
    "        # TODO: nonlinearities\n",
    "        # TODO: Bidirectional\n",
    "\n",
    "        self.fc1=nn.Sequential(\n",
    "             nn.Dropout(p=0.3),\n",
    "             nn.Linear(hsize*2, hsize),\n",
    "            \n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(hsize, outsize),\n",
    "            nn.Softmax(dim=-1)\n",
    "        )\n",
    "\n",
    "        # FIXME: this is a uni-directional LSTM\n",
    "       \n",
    "\n",
    "        self.rnn = nn.LSTM(insize, hsize, 1, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, inputs, seq_lengths, hidden=None):\n",
    "        dim = inputs.shape\n",
    "        \n",
    "        \n",
    "        hin = torch.nn.utils.rnn.pack_padded_sequence(inputs, seq_lengths, batch_first=True)\n",
    "        hout, hidden = self.rnn(hin)\n",
    "        \n",
    "        hout, _= torch.nn.utils.rnn.pad_packed_sequence(hout, batch_first=True) \n",
    "        \n",
    "        hout = hout.contiguous()\n",
    "        hout = hout.view(-1, hout.shape[2])\n",
    "        \n",
    "        fout = self.fc1(F.relu(hout))\n",
    "        \n",
    "        yout = self.fc2(F.relu(fout))\n",
    "        \n",
    "        \n",
    "        yout = yout.view(dim[0], dim[1],-1)\n",
    "        \n",
    "        \n",
    "        return yout, hidden\n",
    "    \n",
    "model = RNN().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "opt = optim.Adam(model.parameters(), lr=0.0015)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "DJubtSt1ozL_"
   },
   "outputs": [],
   "source": [
    "def loss_func(Y_hat, Y, seq_len):\n",
    "    \n",
    "    Y =  torch.argmax(Y, dim=2) \n",
    "    np_array = Y.cpu().numpy()\n",
    "    mask = np.zeros_like(np_array)\n",
    "    for i in range(Y.shape[0]):\n",
    "        mask[i,:seq_len[i]] = 1    \n",
    "    \n",
    "    Y_hat = Y_hat.permute(0,2,1)\n",
    "    \n",
    "    l = criterion(Y_hat, Y)\n",
    "    \n",
    "    mask = n2t(mask)\n",
    "    l = l * mask\n",
    "    mask = mask.cpu().numpy()\n",
    "    total = int(np.count_nonzero(mask))\n",
    "    \n",
    "    l = torch.sum(l)\n",
    "    l = l / total\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "1bD9PZ9LozMC",
    "outputId": "19e0383a-ac35-488f-d8ab-f0f4c36e8488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 2.0813\n"
     ]
    }
   ],
   "source": [
    "EPS = 20\n",
    "train_loss = []\n",
    "eval_loss = []\n",
    "n2t = lambda narr: torch.from_numpy(narr).to(device).float()\n",
    "\n",
    "def eval_model(evaldata, data_len, results=False, predictions= False):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    ypreds = []\n",
    "    seq_length = data_len\n",
    "    for i, sent in enumerate(evaldata):\n",
    "        Xs, Ys = zip(*[load_sent(sent)])\n",
    "        Xs_len = [seq_length[i]]\n",
    "        \n",
    "        Xs, Ys, Xs_len = np.array(Xs), np.array(Ys), np.array(Xs_len)\n",
    "        Xs, Ys = n2t(Xs), n2t(Ys)\n",
    "        \n",
    "        \n",
    "        \n",
    "        with torch.no_grad():\n",
    "            yhat, _ = model(Xs, Xs_len)\n",
    "            ypreds.append(yhat)\n",
    "           \n",
    "            Ys_num = Ys\n",
    "            \n",
    "            loss = loss_func(yhat, Ys_num, Xs_len)\n",
    "            losses.append(loss.item())\n",
    "    print('Eval: %.4f' % np.mean(losses))\n",
    "    \n",
    "    if predictions: \n",
    "        return ypreds\n",
    "    else:\n",
    "        return np.mean(losses)\n",
    "    \n",
    "eval_loss.append(eval_model(eval_sents,eval_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 697
    },
    "colab_type": "code",
    "id": "vuT_pPUaozMF",
    "outputId": "f7379c2b-fa23-4262-bd94-511d0259428d",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[E1/20 - B14961/14986] Train: 1.4576 (elapsed: 9.79s)\n",
      "Eval: 1.4884\n",
      "[E2/20 - B14961/14986] Train: 1.3239 (elapsed: 9.73s)\n",
      "Eval: 1.3524\n",
      "[E3/20 - B14961/14986] Train: 1.3067 (elapsed: 10.04s)\n",
      "Eval: 1.3271\n",
      "[E4/20 - B14961/14986] Train: 1.2833 (elapsed: 9.74s)\n",
      "Eval: 1.3200\n",
      "[E5/20 - B14961/14986] Train: 1.2856 (elapsed: 9.72s)\n",
      "Eval: 1.3143\n",
      "[E6/20 - B14961/14986] Train: 1.2849 (elapsed: 9.74s)\n",
      "Eval: 1.3146\n",
      "[E7/20 - B14961/14986] Train: 1.2880 (elapsed: 9.72s)\n",
      "Eval: 1.3135\n",
      "[E8/20 - B14961/14986] Train: 1.2795 (elapsed: 9.80s)\n",
      "Eval: 1.3145\n",
      "[E9/20 - B14961/14986] Train: 1.2831 (elapsed: 10.08s)\n",
      "Eval: 1.3130\n",
      "[E10/20 - B14961/14986] Train: 1.3050 (elapsed: 9.74s)\n",
      "Eval: 1.3121\n",
      "[E11/20 - B14961/14986] Train: 1.2925 (elapsed: 9.73s)\n",
      "Eval: 1.3131\n",
      "[E12/20 - B14961/14986] Train: 1.2962 (elapsed: 9.79s)\n",
      "Eval: 1.3132\n",
      "[E13/20 - B14961/14986] Train: 1.2860 (elapsed: 9.76s)\n",
      "Eval: 1.3125\n",
      "[E14/20 - B14961/14986] Train: 1.2937 (elapsed: 10.46s)\n",
      "Eval: 1.3160\n",
      "[E15/20 - B14961/14986] Train: 1.2785 (elapsed: 9.73s)\n",
      "Eval: 1.3111\n",
      "[E16/20 - B14961/14986] Train: 1.2784 (elapsed: 9.85s)\n",
      "Eval: 1.3132\n",
      "[E17/20 - B14961/14986] Train: 1.2776 (elapsed: 10.24s)\n",
      "Eval: 1.3161\n",
      "[E18/20 - B14961/14986] Train: 1.2869 (elapsed: 10.86s)\n",
      "Eval: 1.3141\n",
      "[E19/20 - B14961/14986] Train: 1.2804 (elapsed: 10.51s)\n",
      "Eval: 1.3124\n",
      "[E20/20 - B14961/14986] Train: 1.2740 (elapsed: 9.81s)\n",
      "Eval: 1.3110\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for epoch in range(EPS):\n",
    "    model.train()\n",
    "    \n",
    "    t0 = time()\n",
    "    batch_loss = []\n",
    "    for bi in range(0, len(train_inds)-BSIZE, BSIZE):\n",
    "        inds = train_inds[bi:bi+BSIZE]\n",
    "        \n",
    "        inds.sort(key=lambda x: train_len[x], reverse=True)\n",
    "     \n",
    "        # TODO: correct formatting for batchsize >1\n",
    "        Xs, Ys = zip(*[load_sent(sents[ind]) for ind in inds])\n",
    "      \n",
    "        Xs_length = [x.shape[0] for x in Xs]\n",
    "        \n",
    "        max_seq = np.max(Xs_length)\n",
    "        \n",
    "        Xs = list(Xs)\n",
    "        Xs = [np.vstack((x,np.zeros((max_seq-x.shape[0],300)))) for x in Xs]\n",
    "        Ys = [np.vstack((y,np.zeros((max_seq-y.shape[0],8)))) for y in Ys]\n",
    "        \n",
    "       \n",
    "        Xs, Ys = np.array(Xs), np.array(Ys)\n",
    "        \n",
    "        Xs, Ys = n2t(Xs), n2t(Ys)\n",
    "\n",
    "        yhat, _ = model(Xs, Xs_length)\n",
    "        \n",
    "        Ys_num = Ys\n",
    "\n",
    "        opt.zero_grad()\n",
    "        \n",
    "        loss = loss_func(yhat, Ys_num,Xs_length)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "\n",
    "        sys.stdout.write('\\r[E%d/%d - B%d/%d] Train: %.4f ' % (\n",
    "            epoch+1, EPS,\n",
    "            bi+1, len(train_inds),\n",
    "            loss.item(),\n",
    "        ))\n",
    "        batch_loss.append(loss.item())\n",
    "    train_loss.append(np.mean(batch_loss))\n",
    "    sys.stdout.write('(elapsed: %.2fs)\\n' % (time() - t0))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "    loss = eval_model(eval_sents,eval_len)\n",
    "    eval_loss.append(loss)\n",
    "        \n",
    "    # TODO: shuffle train inds\n",
    "    shuffle(train_inds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ee0iQCYsF10u"
   },
   "source": [
    "LSTM Network is used to predict the NER tags of words given a sentence. To imrove the performance of the given model I have added following things -\n",
    "\n",
    "1. The batch size was increase to 20. But with this there was a problem that each sentence had different lengths which the model will not accept. Hence, padding was done to make the all sentences have same length. Initally the padding was done such that each sentence will have length equal to length of sentence with maximum words in entire data set. But this did not give the best results\n",
    "2. CrossEntropy loss criterion was used instead of MSEloss.\n",
    "3. To over come the problem of different lengths of sentences padding was done batch wise. All sentences in a batch will have same length which would be maximum of the sentences in that batch.\n",
    "4. As with padded sentences the Network would see a lot of un wanted padded values which are not very useful. So to over come this I used pytorch's pad_packed_sequence.\n",
    "5. The packing was undone after the LSTM layer.\n",
    "6. I also added a custom loss function because we dont want to calculate loss on the padded indexes.\n",
    "7. I passed the sequence directly to the LSTM, made the LSTM bidirectional\n",
    "8. The output of LSTM was passed through a non linear layer of ReLU\n",
    "9. The output of ReLU was passed through a Dropout layer (0.3), Linear FC layer (input - 256, output - 128)\n",
    "10. The above output was passed through a ReLU layer, Dropout layer (0.2), Linear Layer (input - 128, output - 8) and finally Softmax\n",
    "11. I used the Adam Optimizer, 20 epochs and 0.0015 Learning rate\n",
    "\n",
    "**Hyper Parameter Tuning Steps**\n",
    "\n",
    "1. Increased the number of LSTM layers and added dropout within those layers but it did not give better performance (F1 accuracy - 70)\n",
    "\n",
    "2. The dropout layer was initially made default p=0.5 (F1 accuracy - 71) but reducing it gradually gave better results\n",
    "\n",
    "3. Without adding non linearity ReLU the F1 accuracy was coming to be 71.5 and adding the non linearity helped improve the accuracy.\n",
    "\n",
    "4. The learning rate 0.0005, 0.001 and 0.0015 and the accuracy increased for each higher learning rate but reduced for learning rate beyond 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5y_ciH4eozMH"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 234
    },
    "colab_type": "code",
    "id": "PytMpMwPozMJ",
    "outputId": "9c81a9d3-493e-4cc6-c779-e63f36d6cb26"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAzwAAADFCAYAAACRg/eiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<Figure size 1008x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(14, 3))\n",
    "plt.plot(train_loss, label = 'Train Loss')\n",
    "plt.plot(eval_loss[1:], label = 'Val Loss')\n",
    "plt.legend()\n",
    "plt.show(); plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "YwywKj8_ozML",
    "outputId": "4abe2ac9-d7cb-4e8c-b99a-50fd05815967"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 1.3392\n"
     ]
    }
   ],
   "source": [
    "test_results = eval_model(test_sents, test_len, results=True, predictions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "5YMqWmhMozMR"
   },
   "outputs": [],
   "source": [
    "index2tag = { index: tag for tag, index in tag_lookup.items() }\n",
    "with open('nn.output' , 'w') as fl:\n",
    "    assert len(test_results) == len(test_sents)\n",
    "    for ti, (yhat, schunk) in enumerate(zip(test_results, test_sents)):\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "        for wi, wordinfo in enumerate(schunk.split('\\n')):\n",
    "            word, _, _, tag = wordinfo.split()\n",
    "            taghat = index2tag[np.argmax(yhat[0, wi, :])]\n",
    "\n",
    "            fl.write('%s %s %s\\n' % (word, taghat, tag))\n",
    "        fl.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "mHxBdbTlozMh",
    "outputId": "348106dc-0071-4049-c48f-ec7b8acbe90d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 46665 tokens with 4588 phrases; found: 5648 phrases; correct: 3759.\n",
      "accuracy:  94.45%; precision:  66.55%; recall:  81.93%; FB1:  73.45\n",
      "              LOC: precision:  80.28%; recall:  88.56%; FB1:  84.21  1668\n",
      "             MISC: precision:  68.80%; recall:  77.78%; FB1:  73.02  702\n",
      "              ORG: precision:  60.93%; recall:  76.78%; FB1:  67.94  1661\n",
      "              PER: precision:  57.20%; recall:  81.35%; FB1:  67.18  1617\n"
     ]
    }
   ],
   "source": [
    "!python conlleval.py nn.output "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **STORING RESULTS FOR QUESTION 3**\n",
    "\n",
    "\n",
    "For question 3 we need the prediction of all sentences in train, eval and test. Hence i calculated the predictions for all three data set and stored it in a file as per the order of AIDA dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JWAfkz0ZcQEw",
    "outputId": "cfef6879-4c2c-4913-f288-f2c60a4f4c4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 1.3110\n"
     ]
    }
   ],
   "source": [
    "val_results = eval_model(eval_sents,eval_len, results=True, predictions=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "z9VxJh3fcPyo",
    "outputId": "04bac3b8-0962-4523-ebfe-5b419da8cce5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval: 1.2874\n"
     ]
    }
   ],
   "source": [
    "train_results = eval_model(sents, train_len, results=True, predictions=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "15y_Z86QozMl"
   },
   "outputs": [],
   "source": [
    "index2tag = { index: tag for tag, index in tag_lookup.items() }\n",
    "with open('entire.output' , 'w') as fl:\n",
    "    assert len(train_results) == len(sents)\n",
    "    for ti, (yhat, schunk) in enumerate(zip(train_results, sents)):\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "        for wi, wordinfo in enumerate(schunk.split('\\n')):\n",
    "            word, _, _, tag = wordinfo.split()\n",
    "            taghat = index2tag[np.argmax(yhat[0, wi, :])]\n",
    "\n",
    "            fl.write('%s %s %s\\n' % (word, taghat, tag))\n",
    "        fl.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "CvjQvHLrcpVn"
   },
   "outputs": [],
   "source": [
    "index2tag = { index: tag for tag, index in tag_lookup.items() }\n",
    "with open('entire.output' , 'a') as fl:\n",
    "    assert len(val_results) == len(eval_sents)\n",
    "    for ti, (yhat, schunk) in enumerate(zip(val_results, eval_sents)):\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "        for wi, wordinfo in enumerate(schunk.split('\\n')):\n",
    "            word, _, _, tag = wordinfo.split()\n",
    "            taghat = index2tag[np.argmax(yhat[0, wi, :])]\n",
    "\n",
    "            fl.write('%s %s %s\\n' % (word, taghat, tag))\n",
    "        fl.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "VXQojWo_c3ti"
   },
   "outputs": [],
   "source": [
    "index2tag = { index: tag for tag, index in tag_lookup.items() }\n",
    "with open('entire.output' , 'a') as fl:\n",
    "    assert len(test_results) == len(test_sents)\n",
    "    for ti, (yhat, schunk) in enumerate(zip(test_results, test_sents)):\n",
    "        yhat = yhat.detach().cpu().numpy()\n",
    "        for wi, wordinfo in enumerate(schunk.split('\\n')):\n",
    "            word, _, _, tag = wordinfo.split()\n",
    "            taghat = index2tag[np.argmax(yhat[0, wi, :])]\n",
    "\n",
    "            fl.write('%s %s %s\\n' % (word, taghat, tag))\n",
    "        fl.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Question2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
